{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*Notebook made for the CDH workshop I-Analyzer, October 2022*\n",
        "In this notebook we provide some entrypoints to working with I-Analyzer data in Python. This is by no means a tutorial. Some knowledge of Python is required.\n",
        "\n",
        "You can copy this notebook to your own Google Drive to get an editable file."
      ],
      "metadata": {
        "id": "46NLN5zXceiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "IA0b_rd75wP2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a CSV file attached to this notebook called `queen_king.csv`. We have generated this with the query `queen|king` on the `Times` corpus, filtered for OCR quality 80-100. The download includes the first 10.000 results. See https://ianalyzer.hum.uu.nl/search/times;query=queen%7Cking;$ocr=80:100 for the full results.\n",
        "\n",
        "Try to replicate the download, or come up with an interesting query yourself! You can upload your file using the folder icon in the toolbar on the left-hand side of the screen.\n",
        "\n",
        "Alternatively, you can use the example corpus that Ruben hosts on his GitHub."
      ],
      "metadata": {
        "id": "duCk0_X750iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "import unicodedata\n",
        "import re\n",
        "import networkx\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# set the name of your uploaded file here\n",
        "CSV_FILENAME = 'queen_king.csv'\n",
        "CSV_FILE = 'https://raw.githubusercontent.com/RubenSchalk/textcorpora/main/data/times_sample.csv'"
      ],
      "metadata": {
        "id": "P8hAIYrV6qF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the data into a Pandas dataframe\n",
        "# see https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html for documentation\n",
        "\n",
        "# use CSV_FILE if you wish to read the GitHub file instead\n",
        "full_data = pd.read_csv(CSV_FILENAME)\n",
        "\n",
        "# show some information\n",
        "full_data.info()\n",
        "\n",
        "# take a smaller subset of the data, so we can work faster\n",
        "sample_data = full_data.sample(100)\n",
        "sample_data.head(5)\n",
        "\n"
      ],
      "metadata": {
        "id": "YayB1PIi6gzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example analysis: plotting article length"
      ],
      "metadata": {
        "id": "kxz-4re-GvH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's generate a new colum containing the length (in words) of an article\n",
        "# we use the Spacy package for some common NLP tasks\n",
        "\n",
        "# We use a pre-trained model provided by the Spacy package\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def get_num_words(content):\n",
        "    doc = nlp(content)\n",
        "    return len(doc) # note that this is a very rough tokenizer\n",
        "\n",
        "sample_data['num_words'] = sample_data['content'].apply(get_num_words)\n",
        "\n",
        "# and plot this colum against the publication date\n",
        "sample_data.plot(x='date-pub', y='num_words')"
      ],
      "metadata": {
        "id": "ILaNCOUGG2dG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example analysis: NER"
      ],
      "metadata": {
        "id": "g2O-oIg49G0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will perform Named Entity Recognition on the data. This extracts any named entities from text input, and classifies their types. "
      ],
      "metadata": {
        "id": "pCf0a3bG9JzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to find entities the 'content' field\n",
        "# returns only the text and label\n",
        "def find_entities(content):\n",
        "    doc = nlp(content)\n",
        "    entities = [(ent.text ,ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "sample_data['entities'] = sample_data['content'].apply(find_entities)"
      ],
      "metadata": {
        "id": "aB5YTPAa9TMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If we inspect the data again, we will notice an 'entities' column containing all detected entities\n",
        "sample_data.head()\n",
        "\n",
        "# We can ask what the various labels mean:\n",
        "spacy.explain('NORP')\n",
        "spacy.explain('GPE')\n",
        "\n",
        "# Let's see if some entities occur more than others\n",
        "all_ents = sample_data['entities'].tolist()\n",
        "flattened_ents = [ent for row_ents in all_ents for ent in row_ents]\n",
        "\n",
        "entity_counts = Counter(flattened_ents)\n",
        "entity_counts.most_common(25)"
      ],
      "metadata": {
        "id": "i07wU67wBKn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Spacy's built-in visualizer to show NER in the text\n",
        "from spacy import displacy\n",
        "\n",
        "# We take a random sample of size 1 and select the text\n",
        "text = sample_data.sample(1).iloc[0]['content']\n",
        "doc = nlp(text)\n",
        "sentence_spans = list(doc.sents)\n",
        "\n",
        "displacy.render(sentence_spans, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "id": "WNT1Q22jDOY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# There is stuff available in the Spacy nlp() object:\n",
        "displacy.render(sentence_spans, style='dep', jupyter=True, options={'compact': True})"
      ],
      "metadata": {
        "id": "Q3J9yTzUrTIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example analysis: N-gram visualization"
      ],
      "metadata": {
        "id": "krCEQHWFs_bJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we use the [nltk](https://www.nltk.org/) package to generate N-grams.\n",
        "Heavily inspired by and borrowed from https://towardsdatascience.com/from-dataframe-to-n-grams-e34e29df3460\n"
      ],
      "metadata": {
        "id": "kgFAXPYPtDXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add any stopwords you wish to exclude from the ngrams to this list\n",
        "ADDITIONAL_STOPWORDS = []"
      ],
      "metadata": {
        "id": "xCtS-ktrteHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_clean(text):\n",
        "  \"\"\"\n",
        "  A simple function to clean up the data. All the words that\n",
        "  are not designated as a stop word is then lemmatized after\n",
        "  encoding and basic regex parsing are performed.\n",
        "  \"\"\"\n",
        "  wnl = nltk.stem.WordNetLemmatizer()\n",
        "  stopwords = nltk.corpus.stopwords.words('english') + ADDITIONAL_STOPWORDS\n",
        "  text = (unicodedata.normalize('NFKD', text)\n",
        "    .encode('ascii', 'ignore')\n",
        "    .decode('utf-8', 'ignore')\n",
        "    .lower())\n",
        "  words = re.sub(r'[^\\w\\s]', '', text).split()\n",
        "  return [wnl.lemmatize(word) for word in words if word not in stopwords]\n",
        "\n",
        "# apply the cleaning functions\n",
        "words = basic_clean(''.join(str(sample_data['content'].tolist())))"
      ],
      "metadata": {
        "id": "IyInF_BrtRHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bigrams\n",
        "bigrams = pd.Series(nltk.ngrams(words, 2)).value_counts()\n",
        "bigrams[:10]"
      ],
      "metadata": {
        "id": "AeSI_uTYuU7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# N-grams\n",
        "N = 3\n",
        "# bigrams\n",
        "ngrams = pd.Series(nltk.ngrams(words, N)).value_counts()\n",
        "ngrams[:10]"
      ],
      "metadata": {
        "id": "EDATnUEUupcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize our bigrams using [NetworkX](https://networkx.org/).\n",
        "Inspired by/borrowed from https://www.earthdatascience.org/courses/use-data-open-source-python/intro-to-apis/calculate-tweet-word-bigrams/"
      ],
      "metadata": {
        "id": "4aF3Mgqevccg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dictionary of bigrams and their counts\n",
        "bigrams_dict = bigrams[:50].to_dict()\n",
        "\n",
        "# Create network plot \n",
        "graph = networkx.Graph()\n",
        "\n",
        "# Create connections between nodes\n",
        "for k, v in bigrams_dict.items():\n",
        "    graph.add_edge(k[0], k[1], weight=(v * 10))"
      ],
      "metadata": {
        "id": "0nsf_9hdvkqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "pos = networkx.spring_layout(graph, k=2)\n",
        "\n",
        "# Plot networks\n",
        "networkx.draw_networkx(graph, pos,\n",
        "                 font_size=16,\n",
        "                 width=3,\n",
        "                 edge_color='grey',\n",
        "                 node_color='purple',\n",
        "                 with_labels = False,\n",
        "                 ax=ax)\n",
        "\n",
        "# Create offset labels\n",
        "for key, value in pos.items():\n",
        "    x, y = value[0]+.135, value[1]+.045\n",
        "    ax.text(x, y,\n",
        "            s=key,\n",
        "            horizontalalignment='center', fontsize=13)\n",
        "    \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uWk5TNw2wQ20"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}